# Add this to a new cell in your notebook
import json
from datetime import datetime
from rag_system import RAGSystem
from llama3 import LlamaModel

class AIWaiter:
    def __init__(self, menu_file_path, model_name='llama3.1:latest'):
        """
        Initialize AI waiter chatbot (Vietnamese) with RAG + Llama 3
        Args:
            menu_file_path (str): path to menu json file
            model_name (str): Ollama model identifier
        """
        self.menu_file_path = menu_file_path
        self.model_name = model_name

        # Components
        self.rag = None
        self.llama = None

        # Chat history
        self.conversation_history = []

        print('Initialize Chatbot...')
        self._initialize_systems()

    def _initialize_systems(self):
        """
        Init RAG + Llama model using Ollama
        """
        try:
            # Load RAG 
            print('Loading RAG system...')
            self.rag = RAGSystem(
                menu_file_path=self.menu_file_path
            )
            # Load Llama system 
            self.llama = LlamaModel(model_name=self.model_name)
            success = self.llama.load_model()
            
            if not success:
                raise Exception("Failed to load Llama model via Ollama")

            print(f'\u2705 Chatbot initialization complete!')
        except Exception as e:
            print(f'247c Error initializing chatbot: {e}')
            raise

    def _get_relevant_context(self, user_message, top_k=3): 
        """
        Get relevant context from RAG system 
        """
        try: 
            context = self.rag.get_context_for_llms(user_message, top_k= top_k) 
            return context

        except Exception as e: 
            print(f'\u274c Error when loading context for LLms')
            return 'No information available at the moment '
    def _create_system_prompt(self, context):
        """
        Create system prompt with menu context
        """
        system_prompt = f"""B·∫°n l√† "Linh", m·ªôt nh√¢n vi√™n ph·ª•c v·ª• th√¢n thi·ªán t·∫°i nh√† h√†ng Vi·ªát Nam.

        TH√îNG TIN MENU C√ì S·∫¥N:
        {context}

        NHI·ªÜM V·ª§:
        - Gi√∫p kh√°ch h√†ng t√¨m hi·ªÉu menu v√† ƒë·∫∑t m√≥n
        - T∆∞ v·∫•n m√≥n ƒÉn d·ª±a tr√™n th√¥ng tin menu c√≥ s·∫µn
        - Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ m√≥n ƒÉn, nguy√™n li·ªáu, gi√° c·∫£
        - G·ª£i √Ω m√≥n ƒÉn ph√π h·ª£p v·ªõi s·ªü th√≠ch kh√°ch h√†ng

        PHONG C√ÅCH:
        - Th√¢n thi·ªán, nhi·ªát t√¨nh 
        - Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n
        - Gi·ªØ c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin
        - H·ªèi th√™m ƒë·ªÉ hi·ªÉu r√µ nhu c·∫ßu kh√°ch h√†ng

        CH√ö √ù:
        - Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ menu c√≥ s·∫µn ·ªü tr√™n
        - N·∫øu kh√¥ng c√≥ th√¥ng tin v·ªÅ m√≥n n√†o ƒë√≥, h√£y th√†nh th·∫≠t n√≥i v√† g·ª£i √Ω m√≥n kh√°c
        - Lu√¥n ƒë·ªÅ c·∫≠p gi√° c·∫£ khi gi·ªõi thi·ªáu m√≥n
        - H·ªèi v·ªÅ s·ªü th√≠ch, ng√¢n s√°ch n·∫øu c·∫ßn ƒë·ªÉ t∆∞ v·∫•n t·ªët h∆°n"""

        return system_prompt

    def _clean_response(self, response):
        """
        Clean and format the AI response
        """
        if not response:
            return "Xin l·ªói, t√¥i kh√¥ng hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ h·ªèi l·∫°i kh√¥ng?"
        
        # Remove any unwanted patterns or clean up
        cleaned = response.strip()
        
        # Ensure it's not too long
        if len(cleaned) > 500:
            sentences = cleaned.split('.')
            cleaned = '. '.join(sentences[:3]) + '.'
        
        return cleaned

    def chat(self, user_message, max_new_tokens=300, temperature=0.7):
        """
        Main chat function that combines RAG + LLM
        Args:
            user_message (str): User's message
            max_new_tokens (int): Maximum tokens to generate
            temperature (float): Sampling temperature

        Returns:
            str: chatbot response
        """
        try:
            # Get relevant context from RAG
            context = self._get_relevant_context(user_message, top_k=3)

            # Create system prompt with context
            system_prompt = self._create_system_prompt(context)

            # Get chatbot response using Ollama
            response = self.llama.chat(
                user_message=user_message,
                system_message=system_prompt,
                max_new_tokens=max_new_tokens,
                temperature=temperature
            )

            # Clean response
            response = self._clean_response(response)

            # Save to conversation history
            self.conversation_history.append({
                'user': user_message,
                'assistant': response,
                'timestamp': datetime.now().isoformat(),
                'context_used': context[:100] + '...' if len(context) > 100 else context
            })

            return response

        except Exception as e:
            print(f'247c Error in chat: {e}')
            return "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë k·ªπ thu·∫≠t. B·∫°n c√≥ th·ªÉ h·ªèi l·∫°i kh√¥ng? üòÖ"

    def clear_conversation_history(self):
        """Clear conversation history"""
        self.conversation_history = []
        print("Conversation history cleared")

    def save_conversation(self, filename):
        """Save conversation history to file"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"üíæ Conversation saved to {filename}")
        except Exception as e:
            print(f"\247c Error saving conversation: {e}")

    def get_stats(self):
        """Get chatbot statistics"""
        rag_stats = self.rag.get_stats() if self.rag else {}
        return {
            'conversations': len(self.conversation_history),
            'model_name': self.model_name,
            'rag_stats': rag_stats
        }
